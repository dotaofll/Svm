# Introduce SVM

Consider a two-class classfication problem using linear models of the form

<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;y(\mathbf{x})=\omega^{T}&space;f(\mathbf{x})&plus;b&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;y(\mathbf{x})=\omega^{T}&space;f(\mathbf{x})&plus;b&space;$$" title="$$ y(\mathbf{x})=\omega^{T} f(\mathbf{x})+b $$" /></a>


function <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$f$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$f$" title="$f$" /></a> denotes a fixed feature space transformation.In the form `(1-1)` $\omega=\left(\omega_{1} ; \omega_{2} ; \ldots ; \omega_{d} ;\right)$ is the normal vector which decides the direction of the hyperplane. $b$ is the bias which defines the distance between the origin and the hyperplane.

The training dataset `D` comprises N input vectors <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$x=\left(x_{1}&space;;&space;x_{2}&space;;&space;\ldots&space;;&space;x_{d}&space;;\right)$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$x=\left(x_{1}&space;;&space;x_{2}&space;;&space;\ldots&space;;&space;x_{d}&space;;\right)$" title="$x=\left(x_{1} ; x_{2} ; \ldots ; x_{d} ;\right)$" /></a>  with corresponding target values <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$r=\left(r_{1}&space;;&space;r_{2}&space;;&space;\ldots&space;;&space;r_{m}\right)$&space;where&space;$r_{n}&space;\in\{-1,1\}$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$r=\left(r_{1}&space;;&space;r_{2}&space;;&space;\ldots&space;;&space;r_{m}\right)$&space;where&space;$r_{n}&space;\in\{-1,1\}$" title="$r=\left(r_{1} ; r_{2} ; \ldots ; r_{m}\right)$ where $r_{n} \in\{-1,1\}$" /></a> where <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$r_{n}&space;\in\{-1,1\}$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$r_{n}&space;\in\{-1,1\}$" title="$r_{n} \in\{-1,1\}$" /></a>. And new data points $x$ are classfied according to the sign of $y(\mathbf{x})$.

The distance that any data points $x$ to decision surface can represented by

<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;\frac{t_{n}&space;y\left(\mathbf{x}_{n}\right)}{\|\mathbf{w}\|}=\frac{t_{n}\left(\mathbf{w}^{\mathrm{T}}&space;\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)&plus;b\right)}{\|\mathbf{w}\|}&space;(1-2)&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;\frac{t_{n}&space;y\left(\mathbf{x}_{n}\right)}{\|\mathbf{w}\|}=\frac{t_{n}\left(\mathbf{w}^{\mathrm{T}}&space;\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)&plus;b\right)}{\|\mathbf{w}\|}&space;(1-2)&space;$$" title="$$ \frac{t_{n} y\left(\mathbf{x}_{n}\right)}{\|\mathbf{w}\|}=\frac{t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)}{\|\mathbf{w}\|} (1-2) $$" /></a>



The margin is given by the perpendicular distance to the cloest point $\mathbf{X}_{n}$ from the data set,and we wish to optimize the parameters $\omega$ and $\mathbf{b}$ in order to maximize this distance.Thus the maximum margin solution is found by solving 

<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;\underset{\mathbf{w},&space;b}{\arg&space;\max&space;}\left\{\frac{1}{\|\mathbf{w}\|}&space;\min&space;_{n}\left[t_{n}\left(\mathbf{w}^{\mathrm{T}}&space;\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)&plus;b\right)\right]\right\}&space;(1-3)&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;\underset{\mathbf{w},&space;b}{\arg&space;\max&space;}\left\{\frac{1}{\|\mathbf{w}\|}&space;\min&space;_{n}\left[t_{n}\left(\mathbf{w}^{\mathrm{T}}&space;\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)&plus;b\right)\right]\right\}&space;(1-3)&space;$$" title="$$ \underset{\mathbf{w}, b}{\arg \max }\left\{\frac{1}{\|\mathbf{w}\|} \min _{n}\left[t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)\right]\right\} (1-3) $$" /></a>

We assume that the svm can correctly classify the dataset.Given data point $\left(x_{i}, y_{i}\right) \in D$ there is the following expression:
<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;\left\{\begin{array}{ll}{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b>0,}&space;&&space;{y_{i}=&plus;1}&space;\\&space;{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b<0,}&space;&&space;{y_{i}=-1}\end{array}\right.&space;(1-4)&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;\left\{\begin{array}{ll}{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b>0,}&space;&&space;{y_{i}=&plus;1}&space;\\&space;{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b<0,}&space;&&space;{y_{i}=-1}\end{array}\right.&space;(1-4)&space;$$" title="$$ \left\{\begin{array}{ll}{\omega^{T} \boldsymbol{\phi}(x_{i})+b>0,} & {y_{i}=+1} \\ {\omega^{T} \boldsymbol{\phi}(x_{i})+b<0,} & {y_{i}=-1}\end{array}\right. (1-4) $$" /></a>

but here is a constraint:

<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;\left\{\begin{array}{ll}{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b>&plus;1,}&space;&&space;{y_{i}=&plus;1}&space;\\&space;{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b<-1,}&space;&&space;{y_{i}=-1}\end{array}\right.&space;(1-5)&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;\left\{\begin{array}{ll}{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b>&plus;1,}&space;&&space;{y_{i}=&plus;1}&space;\\&space;{\omega^{T}&space;\boldsymbol{\phi}(x_{i})&plus;b<-1,}&space;&&space;{y_{i}=-1}\end{array}\right.&space;(1-5)&space;$$" title="$$ \left\{\begin{array}{ll}{\omega^{T} \boldsymbol{\phi}(x_{i})+b>+1,} & {y_{i}=+1} \\ {\omega^{T} \boldsymbol{\phi}(x_{i})+b<-1,} & {y_{i}=-1}\end{array}\right. (1-5) $$" /></a>

There are some data points in the dataset which the closest to the decision surface that make the form (1-5) equal sign.Such points are also called the support vector.And the so called Margin is the distance between the form (1-5):
<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;\gamma=\frac{2}{\|\omega\|}&space;(1-6)&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;\gamma=\frac{2}{\|\omega\|}&space;(1-6)&space;$$" title="$$ \gamma=\frac{2}{\|\omega\|} (1-6) $$" /></a>

The optimization problem then simply requires that we maximize <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$|&space;|&space;\mathbf{w}|&space;|^{-1}$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$|&space;|&space;\mathbf{w}|&space;|^{-1}$" title="$| | \mathbf{w}| |^{-1}$" /></a>,which is equivalent to minimizing <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$\|\mathbf{w}\|^{2}$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$\|\mathbf{w}\|^{2}$" title="$\|\mathbf{w}\|^{2}$" /></a>,and so we have to solve the optimization problem <a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$\min&space;_{\omega,&space;b}&space;\frac{1}{2}\|\omega\|^{2}$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$\min&space;_{\omega,&space;b}&space;\frac{1}{2}\|\omega\|^{2}$$" title="$$\min _{\omega, b} \frac{1}{2}\|\omega\|^{2}$$" /></a> subject to 

<a href="https://www.codecogs.com/eqnedit.php?latex=\fn_cm&space;$$&space;t_{n}\left(\mathbf{w}^{\mathrm{T}}&space;\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)&plus;b\right)&space;\geqslant&space;1,&space;\quad&space;n=1,&space;\ldots,&space;N&space;(1-7)&space;$$" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;$$&space;t_{n}\left(\mathbf{w}^{\mathrm{T}}&space;\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)&plus;b\right)&space;\geqslant&space;1,&space;\quad&space;n=1,&space;\ldots,&space;N&space;(1-7)&space;$$" title="$$ t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right) \geqslant 1, \quad n=1, \ldots, N (1-7) $$" /></a>

so until now the problem has changed.

