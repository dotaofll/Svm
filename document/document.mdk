Title         : Introduce the support vector machine
Author        : __xxlucas__
Logo          : Yuzhe Liu

[TITLE]


[reference manual]: http://research.microsoft.com/en-us/um/people/daan/madoko/doc/reference.html  "Madoko reference manual"



# Introduce SVM

Consider a two-class classfication problem using linear models of the form

$$
y(\mathbf{x})=\omega^{T} f(\mathbf{x})+b(1-1)
$$

function $f$ denotes a fixed feature space transformation.In the form `(1-1)` $\omega=\left(\omega_{1} ; \omega_{2} ; \ldots ; \omega_{d} ;\right)$ is the normal vector which decides the direction of the hyperplane. $b$ is the bias which defines the distance between the origin and the hyperplane.

The training dataset `D` comprises N input vectors $x=\left(x_{1} ; x_{2} ; \ldots ; x_{d} ;\right)$  with corresponding target values $r=\left(r_{1} ; r_{2} ; \ldots ; r_{m}\right)$ where $r_{n} \in\{-1,1\}$. And new data points $x$ are classified according to the sign of $y(\mathbf{x})$.

The distance that any data points $x$ to decision surface can represented by

$$
\frac{t_{n} y\left(\mathbf{x}_{n}\right)}{\|\mathbf{w}\|}=\frac{t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)}{\|\mathbf{w}\|}   (1-2)
$$

The margin is given by the perpendicular distance to the closest point $\mathbf{X}_{n}$ from the data set,and we wish to optimize the parameters $\omega$ and $\mathbf{b}$ in order to maximize this distance.Thus the maximum margin solution is found by solving 

$$
\underset{\mathbf{w}, b}{\arg \max }\left\{\frac{1}{\|\mathbf{w}\|} \min _{n}\left[t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)\right]\right\} (1-3)
$$

We assume that the svm can correctly classify the dataset.Given data point $\left(x_{i}, y_{i}\right) \in D$ there is the following expression:

$$
\left\{\begin{array}{ll}{\omega^{T} \boldsymbol{\phi}(x_{i})+b>0,} & {y_{i}=+1} \\ {\omega^{T} \boldsymbol{\phi}(x_{i})+b<0,} & {y_{i}=-1}\end{array}\right.  (1-4)
$$

but here is a constraint:

$$
\left\{\begin{array}{ll}{\omega^{T} \boldsymbol{\phi}(x_{i})+b>+1,} & {y_{i}=+1} \\ {\omega^{T} \boldsymbol{\phi}(x_{i})+b<-1,} & {y_{i}=-1}\end{array}\right. (1-5)
$$

There are some data points in the dataset which the closest to the decision surface that make the form (1-5) equal sign.Such points are also called the support vector.And the so called Margin is the distance between the form (1-5):

$$
\gamma=\frac{2}{\|\omega\|} (1-6)
$$

The optimization problem then simply requires that we maximize $| | \mathbf{w}| |^{-1}$,which is equivalent to minimizing $\|\mathbf{w}\|^{2}$,and so we have to solve the optimization problem $\underset{\mathbf{w}, b}{\arg \min } \frac{1}{2}\|\mathbf{w}\|^{2}$ subject to 

$$
t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right) \geqslant 1, \quad n=1, \ldots, N (1-7)
$$

so until now the problem has changed.

To optimize this problem we need to understand the extremum with a constraint and a method to handle it.That`s Lagrangian multiplier.

The Lagrangian multiplier is used to solve such problem which is that is general optimization problems with equality constraints given bellowï¼š

$$
\begin{array}{l}{\min f(\mathbf{x})} \\ {\text {s.t. } g_{j}(\mathbf{x}) \leq 0(j=1,2, \cdots, m)} \\ {h_{k}(\mathbf{x})=0(k=1,2, \cdots, l)}\end{array} (1-8)
$$

but here is an error in form (1-8).That`s this form is a inequality constraints $g_{j}(\mathbf{x}) \leq 0(j=1,2, \cdots, m)$.So if we transform such inequality problem to equality problem the Lagrangian multiplier will make it.

Let`s touch a simple example:

$$
\begin{array}{c}{\min f(x)} \\ {\text { s. } t . g_{1}(x)=a-x \leq 0} \\ {g_{2}(x)=x-b \leq 0}\end{array}
$$

For each constraint $g_{1}$ and $g_{2}$ add a relaxation variable $a_{1}^{2}$,$b_{1}^{2}$.
so form(1-8)`s constraints will be transformed to:

$$
\begin{array}{l}{h_{1}\left(x, a_{1}\right)=g_{1}(x)+a_{1}^{2}=a-x+a_{1}^{2}=0} \\ {h_{2}\left(x, b_{1}\right)=g_{2}(x)+b_{1}^{2}=x-b+b_{1}^{2}=0}\end{array}
$$

The inequality constraint is transformed to equality constraint.So we can use the Lagrangian multiplier to solve this.
Here is the equivalent constraint optimization problem:

$$
\begin{array}{l}{\min f\left(x_{1}, x_{2}, \ldots, x_{n}\right)} \\ {\text {s.t.h}_{k}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=0}\end{array}
$$


Define the equation below:

$$
L(x,\lambda) = f(x) + \sum_{j=1}^{m} \lambda_{j} h_{k}(\mathbf{x})
$$

function $L(x,\lambda)$ is called the Lagrangian multiplier and the parameter $\lambda$ is called the multiplier.Do a partial derivative to the equation and we can get a simultaneous equations:

$$
\left\{\begin{array}{l}{\frac{\partial L}{\partial x_{i}}=0(i=1,2, \cdots, n)} \\ {\frac{\partial L}{\partial \lambda_{k}}=0(j=1,2, \cdots, m)}\end{array}\right.
$$

The obtained solution is a possible extreme point. Since we use the necessary conditions, whether it is an extreme point or not needs to be tested according to the specific conditions of the problem itself. This equation group is called the extreme value requirement of the equality constraint.

Let`s come back to the example that mentioned above.When we convert a inequality constraint to a equality constraint,we can use Lagrangian function the solve this.

$$
L\left(x, a_{1}, b_{1}, \mu_{1}, \mu_{2}\right)=f(x)+\mu_{1}\left(a-x+a_{1}^{2}\right)+\mu_{2}\left(x-b+b_{1}^{2}\right)
$$

We then solve the problem by the equality constraint optimization problem (extreme value necessary condition), simultaneous equation:

$$
\left\{\begin{array}{l}{\frac{\partial F}{\partial x}=\frac{\partial f}{\partial x}+\mu_{1} \frac{d g_{1}}{d x}+\mu_{2} \frac{d g_{2}}{d x}=\frac{d f}{d x}-\mu_{1}+\mu_{2}=0} \\ {\frac{\partial F}{\partial \mu_{1}}=g_{1}+a_{1}^{2}=0, \quad \frac{\partial F}{\partial \mu_{2}}=g_{2}+b_{1}^{2}=0} \\ {\frac{\partial F}{\partial a_{1}}=2 \mu_{1} a_{1}=0, \quad \frac{\partial F}{\partial b_{1}}=2 \mu_{2} b_{1}=0} \\ {\mu_{1} \geq 0, \quad \mu_{2} \geq 0}\end{array}\right.
$$

Thus, the equations (extremely necessary) are converted into:

$$
\left\{\begin{array}{l}{\frac{d f}{d x}+\mu_{1} \frac{d g_{1}}{d x}+\mu_{2} \frac{d g_{2}}{d x}=0} \\ {\mu_{1} g_{1}(x)=0, \mu_{2} g_{2}(x)=0} \\ {\mu_{1} \geq 0, \mu_{2} \geq 0}\end{array}\right.
$$

This is a one-time situation. Similarly, for multiple multiple inequality constraints

$$
\begin{array}{l}{\min f(\mathbf{x})} \\ {\text {s.t.} g_{j}(\mathbf{x}) \leq 0(j=1,2, \cdots, m)}\end{array}
$$

and we have

$$
\left\{\begin{array}{l}{\frac{\partial f\left(x^{*}\right)}{\partial x_{i}}+\sum_{j=1}^{m} \mu_{j} \frac{\partial g_{j}\left(x^{*}\right)}{\partial x_{i}}=0(i=1,2, \ldots, n)} \\ {\mu_{j} g_{j}\left(x^{*}\right)=0(j=1,2, \ldots, m)} \\ {\mu_{j} \geq 0(j=1,2, \ldots, m)}\end{array}\right.
$$

That is so called KKT condition.

