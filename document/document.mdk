Title         : Introduce the support vector machine
Author        : __xxlucas__
Logo          : Yuzhe Liu

[TITLE]

# Madoko 

Madoko is a fast markdown processor for writing professional articles
with a focus on simplicity and plain text readability.

* Read the [reference manual].
* Explore the upper-right toolbox menu to discover how Markdown works. 
* `Alt-Q` reformats the current paragraph.

Enjoy!

[reference manual]: http://research.microsoft.com/en-us/um/people/daan/madoko/doc/reference.html  "Madoko reference manual"



# Introduce SVM

Consider a two-class classfication problem using linear models of the form

$$
y(\mathbf{x})=\omega^{T} f(\mathbf{x})+b(1-1)
$$

function $f$ denotes a fixed feature space transformation.In the form `(1-1)` $\omega=\left(\omega_{1} ; \omega_{2} ; \ldots ; \omega_{d} ;\right)$ is the normal vector which decides the direction of the hyperplane. $b$ is the bias which defines the distance between the origin and the hyperplane.

The training dataset `D` comprises N input vectors $x=\left(x_{1} ; x_{2} ; \ldots ; x_{d} ;\right)$  with corresponding target values $r=\left(r_{1} ; r_{2} ; \ldots ; r_{m}\right)$ where $r_{n} \in\{-1,1\}$. And new data points $x$ are classified according to the sign of $y(\mathbf{x})$.

The distance that any data points $x$ to decision surface can represented by

$$
\frac{t_{n} y\left(\mathbf{x}_{n}\right)}{\|\mathbf{w}\|}=\frac{t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)}{\|\mathbf{w}\|}   (1-2)
$$

The margin is given by the perpendicular distance to the closest point $\mathbf{X}_{n}$ from the data set,and we wish to optimize the parameters $\omega$ and $\mathbf{b}$ in order to maximize this distance.Thus the maximum margin solution is found by solving 

$$
\underset{\mathbf{w}, b}{\arg \max }\left\{\frac{1}{\|\mathbf{w}\|} \min _{n}\left[t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)\right]\right\} (1-3)
$$

We assume that the svm can correctly classify the dataset.Given data point $\left(x_{i}, y_{i}\right) \in D$ there is the following expression:

$$
\left\{\begin{array}{ll}{\omega^{T} \boldsymbol{\phi}(x_{i})+b>0,} & {y_{i}=+1} \\ {\omega^{T} \boldsymbol{\phi}(x_{i})+b<0,} & {y_{i}=-1}\end{array}\right.  (1-4)
$$

but here is a constraint:

$$
\left\{\begin{array}{ll}{\omega^{T} \boldsymbol{\phi}(x_{i})+b>+1,} & {y_{i}=+1} \\ {\omega^{T} \boldsymbol{\phi}(x_{i})+b<-1,} & {y_{i}=-1}\end{array}\right. (1-5)
$$

There are some data points in the dataset which the closest to the decision surface that make the form (1-5) equal sign.Such points are also called the support vector.And the so called Margin is the distance between the form (1-5):

$$
\gamma=\frac{2}{\|\omega\|} (1-6)
$$

The optimization problem then simply requires that we maximize $| | \mathbf{w}| |^{-1}$,which is equivalent to minimizing $\|\mathbf{w}\|^{2}$,and so we have to solve the optimization problem $\underset{\mathbf{w}, b}{\arg \min } \frac{1}{2}\|\mathbf{w}\|^{2}$ subject to 

$$
t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right) \geqslant 1, \quad n=1, \ldots, N (1-7)
$$

so until now the problem has changed.

To optimize this problem we need to understand the extremum with a constraint and a method to handle it.That`s Lagrangian multiplier.

The Lagrangian multiplier is used to solve such problem which is that is general optimization problems with equality constraints given bellowï¼š

$$
\begin{array}{l}{\min f(\mathbf{x})} \\ {\text {s.t. } g_{j}(\mathbf{x}) \leq 0(j=1,2, \cdots, m)} \\ {h_{k}(\mathbf{x})=0(k=1,2, \cdots, l)}\end{array} (1-8)
$$

but here is an error in form (1-8).That`s this form is a inequality constraints $g_{j}(\mathbf{x}) \leq 0(j=1,2, \cdots, m)$.So if we transform such inequality problem to equality problem the Lagrangian multiplier will make it.

Let`s touch a simple example:

$$
\begin{array}{c}{\min f(x)} \\ {\text { s. } t . g_{1}(x)=a-x \leq 0} \\ {g_{2}(x)=x-b \leq 0}\end{array}
$$

For each constraint $g_{1}$ and $g_{2}$ add a relaxation variable $a_{1}^{2}$,$b_{1}^{2}$.
so form(1-8)`s constraints will be transformed to:

$$
\begin{array}{l}{h_{1}\left(x, a_{1}\right)=g_{1}(x)+a_{1}^{2}=a-x+a_{1}^{2}=0} \\ {h_{2}\left(x, b_{1}\right)=g_{2}(x)+b_{1}^{2}=x-b+b_{1}^{2}=0}\end{array}
$$

The inequality constraint is transformed to equality constraint.So we can use the Lagrangian multiplier to solve this.
Here is the equivalent constraint optimization problem:

$$
\begin{array}{l}{\min f\left(x_{1}, x_{2}, \ldots, x_{n}\right)} \\ {\text {s.t.h}_{k}\left(x_{1}, x_{2}, \ldots, x_{n}\right)=0}\end{array}
$$